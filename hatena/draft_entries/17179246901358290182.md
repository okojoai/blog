---
Title: "【論文読み】NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning"
Category:
- 論文読み
- Artificial Intelligence
Draft: true
EditURL: https://blog.hatena.ne.jp/okojoai/okojoai.hatenablog.com/atom/entry/17179246901358290182
---

# NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning

## 1. 3行まとめ

- VLMによる階層的タスク分解とクローズドループ再計画により、デモンストレーション不要のゼロショット長期操作を実現
- ビデオ生成モデルから抽出したオブジェクトキーポイントと人間の手姿勢を運動学的事前知識として利用 → スイッチング機構で最適な参照を選択
- Functional Manipulation Benchmark (FMB) で複雑な組立タスクと自律的エラーリカバリを実証 → 従来手法より高い成功率

## 2. 何が新しいか

**既存手法の限界**  
VLMとビデオ生成モデルはタスク分解や結果予測が可能だが、物理的グラウンディングが不足 → 実環境での実行が困難。既存のビデオベース操作手法は単一ステップの失敗時に回復できない。

**本論文のアプローチ**  
- 高レベル: VLMプランナーがタスクをサブゴールに分解し、ロボット実行をクローズドループで監視 → 失敗時に自律的に再計画
- 低レベル: 生成ビデオからタスク関連オブジェクトのキーポイントと人間の手姿勢を抽出 → スイッチング機構で動的に選択し、オクルージョンや深度誤差に頑健な実行を実現

**結果**  
3つの長期タスクとFMBで評価 → デモなし・学習なしで複雑な組立タスクと器用なエラーリカバリを達成。

## 3. 技術の核心

NovaPlanの階層構造は以下の2層で構成される。

**高レベルVLMプランナー**  
タスク記述 $T$ から、VLMが各ステップ $i$ のサブゴール $g_i$ とビデオプロンプト $p_i$ を生成。実行後、視覚フィードバック $o_i$ を受けて次ステップを決定：

$$
(g_{i+1}, p_{i+1}) = \text{VLM}(T, \{g_j, o_j\}_{j=1}^i)
$$

クローズドループ設計により、単一ステップの失敗を検出 → 自律的に再計画。

**低レベル実行: キーポイント vs 手姿勢のスイッチング**  
生成ビデオから以下を抽出：
- **オブジェクトキーポイント** $K_{\text{obj}}$: GroundedSAMとCoTrackerで追跡
- **手姿勢** $K_{\text{hand}}$: HaMeRで3D手ポーズを推定

ロボットエンドエフェクタの目標位置 $p_{\text{target}}$ は以下で計算：

$$
p_{\text{target}} = \begin{cases}
f_{\text{align}}(K_{\text{obj}}) & \text{if } \sigma(K_{\text{obj}}) < \tau \\
f_{\text{mimic}}(K_{\text{hand}}) & \text{otherwise}
\end{cases}
$$

ここで $\sigma(K_{\text{obj}})$ はキーポイント追跡の不確実性、$\tau$ は閾値。スイッチング機構により、オクルージョンや深度誤差下でも安定した実行を維持。

## 4. 既存手法との比較

| 手法名 | アプローチ | ゼロショット | クローズドループ | 低レベル表現 | 備考 |
|--------|-----------|-------------|-----------------|-------------|------|
| [NovaFlow](https://arxiv.org/abs/2510.08568) | ビデオ生成→フロー追跡 | ✓ | ✗ | 光フロー | オープンループのため失敗時再計画不可 |
| [MOKA](https://arxiv.org/abs/2404.xxxxx) | VLM+視覚プロンプト | ✓ | ✗ | マークベース | キーポイント追跡のみ、手姿勢未利用 |
| [GraspGen](https://arxiv.org/abs/2507.xxxxx) | 拡散モデル把持生成 | ✗ | ✓ | 6-DOF把持 | 把持特化、長期タスク非対応 |
| [Video models (Wiedemer et al.)](https://arxiv.org/abs/2509.20328) | ビデオモデルによる推論 | ✓ | ✗ | ビデオ特徴 | 操作への変換機構なし |
| **NovaPlan (本論文)** | **VLM+ビデオ生成+スイッチング** | **✓** | **✓** | **キーポイント+手姿勢** | **階層的再計画+動的参照選択** |

## 5. 実装コード例

```python
import torch
from transformers import AutoModel
from diffusers import VideoDiffusionPipeline

class NovaPlan:
    def __init__(self, vlm_model, video_gen_model, robot_controller):
        self.vlm = vlm_model  # 例: GPT-4V
        self.video_gen = video_gen_model  # 例: Lumina-Next
        self.robot = robot_controller
        self.keypoint_tracker = CoTracker()
        self.hand_estimator = HaMeR()
        self.tau = 0.1  # スイッチング閾値
    
    def plan_and_execute(self, task_description):
        history = []
        step = 0
        
        while not self.task_complete(history):
            # 高レベルプランニング
            subgoal, video_prompt = self.vlm.generate_subgoal(
                task_description, history
            )
            
            # ビデオ生成
            generated_video = self.video_gen(video_prompt)
            
            # 低レベル実行
            success = self.execute_subgoal(generated_video)
            
            # 観測を記録
            observation = self.robot.get_observation()
            history.append({
                'step': step,
                'subgoal': subgoal,
                'observation': observation,
                'success': success
            })
            step += 1
    
    def execute_subgoal(self, video):
        # オブジェクトキーポイント抽出
        masks = GroundedSAM(video[0])
        K_obj = self.keypoint_tracker(video, masks)
        sigma_obj = self.compute_uncertainty(K_obj)
        
        # 手姿勢推定
        K_hand = self.hand_estimator(video)
        
        # スイッチング機構
        if sigma_obj < self.tau:
            # キーポイント整列モード
            target_pose = self.align_to_keypoints(K_obj)
        else:
            # 手姿勢模倣モード
            target_pose = self.mimic_hand_pose(K_hand)
        
        # ロボット実行
        trajectory = self.plan_trajectory(target_pose)
        return self.robot.execute(trajectory)
    
    def compute_uncertainty(self, keypoints):
        # キーポイント追跡の分散を計算
        return torch.var(keypoints, dim=0).mean()
    
    def align_to_keypoints(self, K_obj):
        # 現在のロボットグリッパー位置とキーポイントを整列
        robot_pos = self.robot.get_ee_pose()
        delta = K_obj[-1] - robot_pos  # 最終フレームのキーポイント
        return robot_pos + delta
    
    def mimic_hand_pose(self, K_hand):
        # 人間の手姿勢をロボット座標系に変換
        return self.retarget_hand_to_robot(K_hand)
```

## 6. 実装上の懸念

**計算コスト**  
- ビデオ生成が各サブゴールで必要 → 1ステップあたり10-30秒の推論時間（Lumina-Next使用時）
- VLMの呼び出し頻度がタスク複雑度に比例 → クローズドループによりステップ数が増加する可能性

**メモリ消費**  
- 生成ビデオ（81フレーム、720p）の保持 + キーポイント追跡 → GPU 16GB以上推奨
- 履歴蓄積によるメモリリーク対策が必要

**深度推定の精度依存**  
- 単眼深度推定（MoGe-2）の誤差が5-10cm → 精密組立タスクでは課題
- オクルージョン下での手姿勢推定精度は未評価（論文内で定量評価なし）

**エッジ実装の困難性**  
- VLM + ビデオ生成モデルの推論がクラウド前提 → レイテンシとプライバシーの課題
- リアルタイム性が求められるタスクには不向き（現状は準静的操作向け）

## 7. よくある質問（FAQ）

**Q1: なぜビデオ生成モデルが必要なのか？直接VLMから行動を生成できないのか？**  
A: VLMは記号的なタスク分解は得意だが、連続的な運動軌道や物理接触の詳細を生成できない。ビデオ生成モデルは暗黙的に物理法則を学習しており、視覚的に妥当な動作シーケンスを生成できる。これを運動学的事前知識として利用することで、物理的グラウンディングを実現している。

**Q2: スイッチング機構の閾値τはどう決定するのか？**  
A: 論文ではτ=0.1を経験的に設定している。キーポイント追跡の分散が小さければ信頼性が高いと判断し、オブジェクト整列モードを選択。分散が大きい（オクルージョンや追跡失敗）場合は手姿勢模倣に切り替える。タスクやセンサ特性に応じた自動調整機構は今後の課題。

**Q3: デモンストレーションが不要とあるが、完全にゼロショットなのか？**  
A: 基盤モデル（VLM、ビデオ生成モデル）は大規模事前学習済みだが、対象タスク固有のデモやファインチューニングは不要。ただし、ロボットの運動学モデルとカメラキャリブレーションは必要。既存手法と異なり、タスクごとのデータ収集やポリシー学習が不要という意味でゼロショット。

**Q4: FMBでの成功率はどの程度か？**  
A: 論文内で具体的な成功率の数値は明記されていないが、複数の組立タスク（例: 蛇口組立、カップ配置）で定性的な成功が報告されている。比較手法（NovaFlow等）より高い成功率と主張されているが、詳細な定量評価はプロジェクトページ参照が必要。

**Q5: クローズドループ再計画はどのタイミングで発動するのか？**  
A: 各サブゴール実行後、VLMが視覚観測を評価し、目標達成度を判定。失敗と判断された場合（例: オブジェクトが落下、位置ずれ）、次のサブゴールを再生成。判定基準はVLMの視覚理解能力に依存するため、誤判定のリスクがある。

## 8. 検討メモ

**自社プロジェクトへの適用案**  
- [ ] 倉庫内ピッキング・配置タスクへの適用検討 → 長期タスク分解とエラーリカバリが有用
- [ ] 既存のMOKAベースシステムとの統合可能性調査 → キーポイント追跡部分を流用

**追試すべき点**  
- [ ] スイッチング閾値τの自動調整手法の開発 → タスク特性に応じた適応的設定
- [ ] ビデオ生成の高速化 → 蒸留モデルや軽量化手法の適用（LightX2V等）
- [ ] FMBでの定量的成功率の再現実験 → 各サブタスクごとの成功率とボトルネック分析

**未解決の疑問**  
- [ ] 動的環境（人やロボットが混在）での適用可能性は？ → ビデオ生成の前提が静的環境
- [ ] 手姿勢推定の失敗モードは？ → オクルージョン下での定量評価が不足
- [ ] VLMのタスク分解能力の限界は？ → 複雑度が一定以上のタスクでの評価が必要

---
> **原論文**: [NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning](https://arxiv.org/abs/2602.20119v1)
