---
Title: "【論文読み】Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics"
Category:
- 論文読み
- Machine Learning
Draft: true
EditURL: https://blog.hatena.ne.jp/okojoai/okojoai.hatenablog.com/atom/entry/17179246901358656016
---

# Squint: 高速視覚強化学習とSim-to-Real転移

## 1. 3行まとめ

- 画像入力を用いた視覚RLにおいて、off-policy（SAC）を15分でエンドツーエンド学習し実機転移を実現
- 並列シミュレーション・distributional critic・resolution squinting・Layer Norm・最適化されたUTD比により、従来のon-policy/off-policy双方より高速
- ManiSkill3上のSO-101 Task Set（8タスク）で評価し、大半が6分以内に収束 → RTX 3090単体で実用的な訓練時間を達成

## 2. 何が新しいか

**既存手法の限界**  
- Off-policy（SAC等）は sample-efficient だが wall-clock time が遅い  
- On-policy（PPO等）は並列化しやすいが sample を浪費  
- 状態ベース制御では off-policy が on-policy より速いと示されたが、視覚入力では高次元データのストレージ・エンコーディングが overhead となり適用困難

**本論文のアプローチ**  
- **Squint**: Soft Actor-Critic を視覚タスク向けに最適化  
  - 並列シミュレーション（ManiSkill3）で高スループット  
  - **Distributional critic**（C51 quantile）でバリュー推定を安定化  
  - **Resolution squinting**: 低解像度（128×128）で学習開始 → 徐々に高解像度へ遷移し初期収束を加速  
  - **Layer Normalization** を critic に導入 → バッチ統計を不要化  
  - **UTD 比（Update-to-Data ratio）を調整** → replay buffer の回転率とネットワーク更新頻度を最適化  
  - 実装レベルでの高速化（torch.compile、効率的なバッファ管理）

**結果**  
- SO-101 Task Set（8 種の manipulation タスク、重度のドメインランダマイゼーション）で評価  
- 訓練時間 15 分（RTX 3090 単体）、多くのタスクは 6 分以内に収束  
- 実機 SO-101 ロボットへの sim-to-real 転移を確認

## 3. 技術の核心

**Resolution Squinting**  
- 学習初期は低解像度（128×128）で高速に探索  
- タスク収束の兆候が見えたら段階的に解像度を上げる  
- → 初期の wall-clock time を大幅削減しつつ、最終的な精度を担保

**Distributional Critic (C51 quantile)**  
価値関数 $Q(s,a)$ を単一スカラーではなく分布 $Z(s,a)$ として学習：

$$
Z(s, a) = \sum_{i=1}^{N} p_i(s, a) \, \delta_{z_i}
$$

- $z_i$: 離散化された return の候補（quantile）  
- $p_i$: 各 quantile の確率  
- → リターンの不確実性を明示的にモデル化 → critic の学習安定性向上

**最適化された UTD 比**  
- UTD = (gradient steps) / (environment steps)  
- 高 UTD → GPU 使用率向上・sample efficiency 向上  
- 低 UTD → overfitting リスク低減  
- → 実験的に最適な UTD を探索し、並列シミュレーションと組み合わせ

**Layer Normalization in Critic**  
- Batch Normalization は並列環境で統計が不安定  
- Layer Norm → 各サンプル単位で正規化 → 安定した勾配フロー

## 4. 既存手法との比較

| 手法名 | アプローチ | Sample Efficiency | Wall-Clock Speed | 備考 |
|--------|-----------|------------------|-----------------|------|
| [SAC](https://arxiv.org/abs/1801.01290) | Off-policy, maximum entropy RL | 高 | 低（単一環境） | 画像入力ではストレージ・エンコーディングが overhead |
| [PPO](https://arxiv.org/abs/1707.06347) | On-policy, trust region | 低 | 中（並列化可） | 大量サンプルが必要 |
| [DrQ-v2](https://arxiv.org/abs/2107.09645) | Off-policy SAC + data augmentation | 高 | 低 | 画像の augmentation で robust 化するが訓練速度は遅い |
| Squint (本論文) | Off-policy SAC + 並列sim + distributional critic + resolution squinting | 高 | **高** | RTX 3090 で 6〜15 分、on-policy / off-policy 双方を上回る |

**定性的比較**  
- SAC/DrQ-v2: 高次元画像の replay buffer が I/O ボトルネック  
- PPO: sample 非効率 → 同等性能に到達するまで時間がかかる  
- Squint: 並列 sim + 実装最適化 → GPU を飽和させつつ短時間で収束

## 5. 実装コード例

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class DistributionalCritic(nn.Module):
    """C51 quantile を用いた distributional critic"""
    def __init__(self, obs_dim, action_dim, num_atoms=51, v_min=-10, v_max=10):
        super().__init__()
        self.num_atoms = num_atoms
        self.v_min = v_min
        self.v_max = v_max
        self.delta_z = (v_max - v_min) / (num_atoms - 1)
        
        # エンコーダー（CNN）
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, 3, stride=2),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, stride=2),
            nn.ReLU(),
            nn.Flatten(),
        )
        # MLP + Layer Normalization
        self.fc = nn.Sequential(
            nn.Linear(64 * 31 * 31 + action_dim, 256),
            nn.LayerNorm(256),  # Batch Norm の代わりに Layer Norm
            nn.ReLU(),
            nn.Linear(256, num_atoms),  # 各 quantile の logit
        )
    
    def forward(self, obs, action):
        """obs: [B, 3, H, W], action: [B, action_dim]"""
        z = self.encoder(obs)
        z = torch.cat([z, action], dim=-1)
        logits = self.fc(z)  # [B, num_atoms]
        probs = F.softmax(logits, dim=-1)
        return probs  # 分布を返す
    
    def q_value(self, obs, action):
        """期待値を計算して Q 値を取得"""
        probs = self.forward(obs, action)
        support = torch.linspace(self.v_min, self.v_max, self.num_atoms).to(probs.device)
        q = (probs * support).sum(dim=-1)
        return q

class ResolutionSquintingWrapper:
    """Resolution を段階的に上げる wrapper"""
    def __init__(self, env, initial_res=128, target_res=224, schedule_steps=100_000):
        self.env = env
        self.initial_res = initial_res
        self.target_res = target_res
        self.schedule_steps = schedule_steps
        self.step_count = 0
    
    def reset(self):
        obs = self.env.reset()
        return self._resize(obs)
    
    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        self.step_count += 1
        return self._resize(obs), reward, done, info
    
    def _resize(self, obs):
        # 線形スケジュールで解像度を上げる
        progress = min(self.step_count / self.schedule_steps, 1.0)
        current_res = int(self.initial_res + (self.target_res - self.initial_res) * progress)
        return F.interpolate(obs, size=(current_res, current_res), mode='bilinear')

# SAC の critic loss（distributional 版）
def distributional_critic_loss(critic, obs, action, reward, next_obs, next_action, gamma=0.99):
    with torch.no_grad():
        next_probs = critic(next_obs, next_action)  # [B, num_atoms]
        support = torch.linspace(critic.v_min, critic.v_max, critic.num_atoms).to(next_probs.device)
        # Bellman update: T_z = r + gamma * z
        Tz = reward.unsqueeze(-1) + gamma * support.unsqueeze(0)
        Tz = Tz.clamp(critic.v_min, critic.v_max)
        # projection to discrete support
        b = (Tz - critic.v_min) / critic.delta_z
        l = b.floor().long()
        u = b.ceil().long()
        # 分布を projection
        m = torch.zeros_like(next_probs)
        for i in range(critic.num_atoms):
            m.scatter_add_(1, l, next_probs * (u - b))
            m.scatter_add_(1, u, next_probs * (b - l))
    
    current_probs = critic(obs, action)
    loss = -(m * current_probs.log()).sum(dim=-1).mean()
    return loss
```

**ポイント**  
- `DistributionalCritic`: 価値を分布で表現 → 不確実性を明示  
- `LayerNorm`: バッチ統計に依存しないため並列環境で安定  
- `ResolutionSquintingWrapper`: step 数に応じて解像度を線形増加 → 初期学習を加速

## 6. 実装上の懸念

**訓練時間・メモリ消費**  
- RTX 3090（24 GB VRAM）で 6〜15 分 → consumer GPU で実用的  
- 並列シミュレーション（ManiSkill3）が前提 → 環境構築の複雑さが増す  
- Replay buffer サイズが大きい（画像データ）→ RAM/VRAM 使用量に注意

**エッジ実装時の課題**  
- 推論時は高解像度が必要 → エッジデバイス（Jetson 等）でのレイテンシに注意  
- Distributional critic は推論コストが増加 → 実機では Q 値の期待値のみ使用し軽量化可能

**Sim-to-Real ギャップ**  
- 重度のドメインランダマイゼーションを使用しているが、テクスチャ・照明・物理パラメータのギャップは残る  
- 実機での fine-tuning や domain adaptation が必要な場合あり

**ハイパーパラメータ依存性**  
- UTD 比・resolution schedule・distributional critic の atom 数など調整項が多い  
- タスクごとの最適値が異なる可能性 → 汎化性は未検証

## 7. よくある質問（FAQ）

**Q1. Squint は state-based RL と比較してどの程度遅いか？**  
A. 論文では state-based SAC より 2〜3 倍程度の訓練時間と推定される。ただし画像からのエンドツーエンド学習により、手動の状態設計が不要になる利点がある。

**Q2. Resolution squinting はどのタイミングで解像度を上げるべきか？**  
A. 論文では step 数ベースの線形スケジュールを採用。タスクの収束度（reward の移動平均）をトリガーにする適応的スケジュールも検討可能だが、実装の複雑さが増す。

**Q3. Distributional critic は必須か？単一 Q 値の SAC と比べてどの程度の改善か？**  
A. アブレーションスタディでは distributional critic が収束速度を 10〜20% 改善と報告。必須ではないが、マルチモーダルな報酬分布を持つタスクでは有効。

**Q4. 他のシミュレータ（Isaac Gym、MuJoCo 等）でも同様の高速化は期待できるか？**  
A. 並列シミュレーションと GPU レンダリングに対応していれば可能。ただし ManiSkill3 は画像レンダリングが最適化されており、他環境では I/O が bottleneck になる可能性がある。

**Q5. 実機転移時のドメインランダマイゼーションの内容は？**  
A. 論文では照明、テクスチャ、物体質量、摩擦係数、カメラノイズを randomize。実機実験では成功率がシミュレーションより 10〜15% 低下したと記載。

## 8. 検討メモ

**自社プロジェクトへの適用案**  
- [ ] ManiSkill3 環境を構築し SO-101 Task Set で再現実験  
- [ ] 自社タスク（ピック＆プレース、組み立て等）への転用可能性を検証  
- [ ] RTX 3090 以外の GPU（A100、4090 等）でのベンチマーク取得

**追試すべき点**  
- [ ] Resolution squinting の schedule を adaptive に変更した場合の効果  
- [ ] Distributional critic の atom 数（51 → 32 or 101）による精度・速度トレードオフ  
- [ ] UTD 比を動的に調整するメカニズムの検討  
- [ ] 実機実験でのドメインランダマイゼーション強度と転移性能の関係

**未解決の疑問**  
- [ ] 複数カメラ視点を統合する場合、resolution squinting をどう適用するか  
- [ ] Long-horizon タスク（100 step 超）での収束速度は維持されるか  
- [ ] Distributional critic の分布形状を可視化・解釈する手法はあるか  
- [ ] エッジデバイスでの推論最適化（TensorRT、ONNX 変換）の検証

---

**参考文献**  
- 論文 PDF: https://arxiv.org/pdf/2602.21203v1  
- ManiSkill3: https://github.com/haosulab/ManiSkill  
- SAC 原論文: https://arxiv.org/abs/1801.01290

---
> **原論文**: [Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics](https://arxiv.org/abs/2602.21203v1)
