---
Title: "【論文読み】Dex4D: Task-Agnostic Point Track Policy for Sim-to-Real Dexterous Manipulation"
Category:
- 論文読み
- Machine Learning
Draft: true
EditURL: https://blog.hatena.ne.jp/okojoai/okojoai.hatenablog.com/atom/entry/17179246901356190140
---

# Dex4D：タスク非依存の4Dポイントトラック操作ポリシー

## 1. 3行まとめ

- シミュレーションで学習したタスク非依存の3Dポイントトラック条件付きポリシー → 実ロボットでゼロショット転移を実現
- 数千オブジェクトの多様なポーズ遷移を"Anypose-to-Anypose"方式で学習 → 実行時に動画生成とオンライン点追跡で任意のタスクに対応
- 実環境でのファインチューニング不要 → 動画から抽出したオブジェクト中心の点軌跡をプロンプトとして与えるだけで多様な器用操作を実行

## 2. 何が新しいか

従来の器用操作学習は、実環境での大規模テレオペレーションデータ収集が高コスト、またはシミュレーション学習でも各タスクごとに環境・報酬設計が必要という課題があった。

本論文はタスク固有の環境設計を避け、シミュレーションでタスク非依存の器用スキルを学習する枠組みを提案。具体的には、数千のオブジェクトとポーズ構成で3Dポイントトラック条件付きポリシーを訓練 → ロボットとオブジェクトの幅広い相互作用空間をカバー → テスト時に柔軟に再構成可能。

実行時は生成動画から抽出した望ましいオブジェクト中心点軌跡をプロンプトとして与え、オンライン点追跡で閉ループ制御を実現 → ゼロショットで実環境タスクに転移。

## 3. 技術の核心

Dex4Dのポリシーは、現在の観測（深度画像とロボット状態）および目標点軌跡を入力として、次の行動を出力する：

$$
\pi_\theta(a_t | o_t, g_t)
$$

- $o_t$：時刻$t$のRGB-D観測およびロボット固有受容状態
- $g_t$：オブジェクト中心座標系で定義された目標3D点軌跡（時系列）
- $a_t$：ロボットの関節角度ターゲット

ポリシーは以下の3段階で構成される：

1. **3D点雲エンコーディング**：深度画像から点群を生成し、PointNetベースのエンコーダで特徴抽出
2. **トランスフォーマベースの時空間集約**：過去の観測履歴と目標軌跡をアテンションで統合
3. **行動デコーダ**：MLPで関節空間の行動を出力

訓練は**Isaac Gym**上でPPOを使用し、報酬は以下で定義される：

$$
r = -\lambda_1 \|\mathbf{p}_{obj} - \mathbf{p}_{goal}\|_2 - \lambda_2 \|\mathbf{q}_{obj} - \mathbf{q}_{goal}\|_{rot} + \lambda_3 \mathbb{1}_{success}
$$

- $\mathbf{p}_{obj}, \mathbf{q}_{obj}$：現在のオブジェクト位置と姿勢
- $\mathbf{p}_{goal}, \mathbf{q}_{goal}$：目標位置と姿勢
- $\|\cdot\|_{rot}$：回転誤差（Kabschアライメントベース）

シミュレーション時はランダムなオブジェクト・初期/目標ポーズをサンプル → ドメインランダマイゼーションで視覚的多様性を確保 → タスク非依存の汎化能力を獲得。

実行時は**CoTracker3**でオンライン点追跡を実施し、軌跡の相対偏差を補正しながら閉ループ制御を継続。

## 4. 既存手法との比較

| 手法名 | アプローチ | sim成功率 | real適用 | 備考 |
|--------|-----------|-----------|----------|------|
| [NovaFlow](https://arxiv.org/abs/2510.08568) | 動画生成ベースの光学フロー誘導 | - | あり | フローは2Dベース、閉ループ不足 |
| [DextraH-G](https://arxiv.org/abs/2410.04758) | ピクセル→行動の幾何ファブリック | - | あり | 把持特化、ポーズ到達タスクは未対応 |
| [GraspGen](https://arxiv.org/abs/2501.03879) | 拡散ベース6-DoF把持 | - | あり | 把持のみ、操作後の配置は範囲外 |
| [UnidexGrasp](https://arxiv.org/abs/2303.00938) | 多様な把持候補生成+RL | 80%台（把持） | 限定的 | 把持後の操作は別途設計が必要 |
| **Dex4D** | タスク非依存3D点軌跡条件付きポリシー | **94.3%**（平均） | **ゼロショット** | 単一ポリシーで把持・配置・回転など多タスク対応 |

Dex4Dは単一のポリシーで多様なタスクをカバーし、実環境でのファインチューニングなしで転移可能な点が差別化要素。

## 5. 実装コード例

```python
import torch
import torch.nn as nn
from pointnet2_ops import pointnet2_utils

class Dex4DPolicy(nn.Module):
    def __init__(self, point_dim=3, proprioception_dim=24, hidden_dim=256):
        super().__init__()
        # PointNetベースの点群エンコーダ
        self.point_encoder = PointNetEncoder(point_dim, hidden_dim)
        
        # 目標軌跡エンコーダ（時系列のポイントトラックを処理）
        self.goal_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=8),
            num_layers=4
        )
        
        # 固有受容情報のエンコーダ
        self.proprio_encoder = nn.Linear(proprioception_dim, hidden_dim)
        
        # 行動デコーダ
        self.action_head = nn.Sequential(
            nn.Linear(hidden_dim * 3, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 24)  # 関節角度（LEAP Hand: 16 + アーム: 8）
        )
    
    def forward(self, depth, proprio, goal_tracks):
        """
        depth: (B, H, W) 深度画像
        proprio: (B, 24) ロボット状態（関節角度・速度）
        goal_tracks: (B, T, N, 3) 目標点軌跡（時刻T、点数N、3D座標）
        """
        # 深度から点群を生成
        points = depth_to_pointcloud(depth)  # (B, N_pts, 3)
        
        # 点群エンコーディング
        point_feat = self.point_encoder(points)  # (B, hidden_dim)
        
        # 目標軌跡をフラット化してエンコード
        B, T, N, _ = goal_tracks.shape
        goal_flat = goal_tracks.reshape(B, T*N, 3)
        goal_feat = self.goal_encoder(goal_flat)  # (B, hidden_dim)
        
        # 固有受容情報のエンコード
        proprio_feat = self.proprio_encoder(proprio)  # (B, hidden_dim)
        
        # 特徴を結合して行動を出力
        combined = torch.cat([point_feat, goal_feat, proprio_feat], dim=-1)
        action = self.action_head(combined)
        return action

class PointNetEncoder(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.conv1 = nn.Conv1d(input_dim, 64, 1)
        self.conv2 = nn.Conv1d(64, 128, 1)
        self.conv3 = nn.Conv1d(128, output_dim, 1)
    
    def forward(self, points):
        # points: (B, N, 3) -> (B, 3, N)
        x = points.transpose(1, 2)
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = self.conv3(x)
        # グローバル特徴量（max pooling）
        x = torch.max(x, dim=-1)[0]  # (B, output_dim)
        return x

def depth_to_pointcloud(depth, intrinsics):
    """深度画像を3D点群に変換（カメラ座標系）"""
    # 実装簡略化のため擬似コード
    # 実際はカメラ内部パラメータを用いてバックプロジェクション
    pass

# PPO訓練ループ（Isaac Gym環境）
def train_ppo(env, policy, num_iterations=10000):
    optimizer = torch.optim.Adam(policy.parameters(), lr=3e-4)
    for iter in range(num_iterations):
        # ロールアウト収集
        states, actions, rewards, dones = collect_rollouts(env, policy)
        
        # GAEでアドバンテージ推定
        advantages = compute_gae(rewards, dones)
        
        # PPOクリップ損失で更新
        loss = compute_ppo_loss(policy, states, actions, advantages)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

## 6. 実装上の懸念

- **訓練コスト**：Isaac Gymで数千並列環境を使用 → GPU 1台（A100）で約24時間の訓練時間。タスク数が増えると線形に増加する可能性
- **ポイントトラッキングの精度依存**：CoTracker3の追跡失敗 → 軌跡のドリフト発生 → ポリシーの性能劣化。論文では成功率が追跡精度に強く相関
- **深度センサの品質**：実環境でRealSense D435を使用 → 反射・透明物体で深度ノイズが増加 → 点群特徴抽出の劣化。ドメインランダマイゼーションで緩和しているが完全ではない
- **動画生成の計算コスト**：Wanモデルでゴール動画を生成 → 1タスクあたり数十秒〜数分。リアルタイム対話的なタスク指示には不向き
- **エッジ実装**：ポリシー推論は軽量（10Hz程度）だが、CoTracker3とSAM2の実行が重い → 専用GPUが必要。組み込み環境では追加の最適化が必須

## 7. よくある質問（FAQ）

**Q1. Dex4Dは事前にタスクを知る必要があるか？**

不要。ポリシーは"Anypose-to-Anypose"で訓練されており、実行時に動画から抽出した点軌跡をプロンプトとして受け取るだけでゼロショット実行可能。タスク固有の報酬設計やファインチューニングは行わない。

**Q2. 実環境でのsim-to-real転移はどのように保証されるか？**

シミュレーション訓練時にドメインランダマイゼーション（照明・テクスチャ・カメラノイズ）を適用。加えてオンライン点追跡で実環境の視覚フィードバックを直接利用 → 分布シフトを緩和。論文では平均77.5%の実環境成功率を報告。

**Q3. どの程度の新規オブジェクトに汎化できるか？**

訓練にはYCBデータセットやObjaverse-XLの数千オブジェクトを使用。未見のオブジェクト（形状・サイズ）にも一定の汎化を示すが、極端に大きい/小さい物体や特殊な物理特性（柔軟・滑りやすい）では性能低下の可能性がある。

**Q4. 他の視覚追跡手法（TAP-Vid、PIPs等）との互換性は？**

アーキテクチャ上、CoTracker3以外の点追跡モデルも利用可能。ただし論文では追跡精度と成功率に強い相関があるため、高精度なトラッカーが推奨される。SAM2のセグメンテーションと組み合わせることで追跡の初期化を改善。

**Q5. マルチモーダル制御（力覚・触覚）への拡張は可能か？**

現状は視覚（RGB-D）と固有受容感覚のみを使用。触覚センサ（GelSight等）の統合は原理的に可能だが、シミュレーションでの触覚モデリングが課題。強化学習の報酬に力制約を追加することで接触動作の改善が期待される。

## 8. 検討メモ

- **自社適用の可能性**：
  - 産業用ピッキング・配置タスクへの適用 → YCBセット以外の自社オブジェクトでの追加訓練が必要か検証
  - 生成動画の代わりに手動デモ動画から点軌跡を抽出できるか試す → ユーザー指示の柔軟性向上
- **追試ポイント**：
  - CoTracker3の追跡失敗ケースを体系的に分析 → どの種類の物体・動作で頻発するかを定量化
  - ドメインランダマイゼーションのハイパーパラメータ感度 → sim-to-real成功率への影響を測定
  - 訓練オブジェクト数と汎化性能のトレードオフ → 数百〜数千オブジェクトでの精度曲線をプロット
- **未解決の疑問**：
  - 複数オブジェクトの同時操作（両手協調）に拡張可能か？ → 現状は単一オブジェクト前提
  - 動的環境（移動する障害物、他エージェント）での閉ループ制御の頑健性は？
  - Wanモデル以外の動画生成手法（Sora、Runway等）との互換性と生成速度の比較

---
> **原論文**: [Dex4D: Task-Agnostic Point Track Policy for Sim-to-Real Dexterous Manipulation](https://arxiv.org/abs/2602.15828v1)
