---
Title: "【論文読み】When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs"
Category:
- 論文読み
- Computer Vision
Draft: true
EditURL: https://blog.hatena.ne.jp/okojoai/okojoai.hatenablog.com/atom/entry/17179246901357573257
---

# When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs

## 1. 3行まとめ

- VLAは視覚ショートカット（データセットバイアス）により言語指示を無視してしまう**counterfactual failures**が頻発する
- LIBERO-CFベンチマークで既存VLAの言語追従能力を評価 → 最新手法でも反事実的なタスクで大幅に性能低下
- Counterfactual Action Guidance（CAG）により、言語非依存VAモジュールとの二分岐推論で視覚ショートカットを抑制 → 訓練不要で$π_{0.5}$の言語追従精度9.7%向上

## 2. 何が新しいか

**既存手法の限界**  
Vision-Language-Action models（VLAs）は言語指示をロボット制御に落とし込む技術として期待されているが、実際にはシーン固有の教師信号が弱い場合に**言語指示を無視**する。訓練データのバイアスにより視覚的ショートカットが学習され、指示内容に関わらず頻出物体や習熟行動を繰り返す。

**本論文のアプローチ**  
- **LIBERO-CF**: VLAの言語追従能力を評価する初の反事実ベンチマーク。視覚的に妥当なLIBEROレイアウトに対して代替指示を割り当て、counterfactual failuresを定量評価
- **Counterfactual Action Guidance（CAG）**: 標準VLA policyと言語非依存Vision-Action（VA）モジュールの二分岐推論。VAモジュールが視覚のみで予測する行動との差分を用いて言語conditioning強度を明示的に調整

**結果**  
- LIBERO-CFで$π_{0.5}$の言語追従精度を9.7%、訓練サンプルが少ないタスクで成功率3.6%向上（訓練不要版）
- VAモデルと組み合わせた場合は各15.5%、8.5%向上
- 実環境でcounterfactual failures 9.4%減、タスク成功率17.2%向上

## 3. 技術の核心

**二分岐推論による言語conditioning強化**  
CAGは以下の3ステップで推論を行う：

1. **標準VLA policy**: $a_{\text{VLA}} = \pi_{\text{VLA}}(o, l)$（観測$o$、言語指示$l$）
2. **言語非依存VA policy**: $a_{\text{VA}} = \pi_{\text{VA}}(o)$（視覚ショートカットのベースライン）
3. **Counterfactual差分による調整**:
   $$a_{\text{CAG}} = a_{\text{VLA}} + \lambda \cdot (a_{\text{VLA}} - a_{\text{VA}})$$
   
$\lambda > 0$は言語conditioningの強度を制御するハイパーパラメータ。  
$(a_{\text{VLA}} - a_{\text{VA}})$が正の場合 → VLAが言語指示に基づいて視覚ショートカットから逸脱 → その方向を増幅。  
差分が小さい場合 → 視覚ショートカットに依存している可能性 → 標準VLAに近い挙動。

**訓練不要版（VAモデル未使用）**  
VAモジュールが利用できない場合、$a_{\text{VA}}$を過去の行動履歴から推定：
$$a_{\text{VA}} \approx \text{mode}(a_{t-k}, \dots, a_{t-1})$$
頻出行動をショートカットのプロキシとして利用 → 追加学習なしで適用可能。

## 4. 既存手法との比較

| 手法名 | アプローチ | 言語追従精度（LIBERO-CF） | 訓練コスト | 備考 |
|--------|-----------|--------------------------|-----------|------|
| [π0](https://arxiv.org/abs/2410.00434) | Flow-based VLA、事前学習済み | ベースライン | 大規模事前学習 | counterfactual failures頻発 |
| [π0.5](https://arxiv.org/abs/2501.09850) | π0のopen-world版 | ベースライン | 大規模事前学習 | 視覚ショートカット依存 |
| Fine-tuning VLA | 既存VLAのfine-tuning | 改善限定的 | タスク別追加学習 | データセットバイアス増幅リスク |
| **CAG (訓練不要)** | 二分岐推論、履歴ベースVA推定 | +9.7% | なし | plug-and-play、既存VLAに統合可能 |
| **CAG (VAモデル有)** | 二分岐推論、明示的VA学習 | +15.5% | VAモジュール学習のみ | 言語conditioning強化、アーキテクチャ変更不要 |

※精度はπ0.5ベースライン比の相対向上率（LIBERO-CF言語追従精度）

## 5. 実装コード例

```python
import torch
import torch.nn as nn
from collections import deque

class CounterfactualActionGuidance:
    def __init__(self, vla_policy, va_policy=None, lambda_cag=0.5, history_size=10):
        """
        Args:
            vla_policy: 事前学習済みVLAモデル (obs, lang) -> action
            va_policy: 言語非依存VAモデル (obs) -> action。Noneなら履歴ベース推定
            lambda_cag: counterfactual差分の重み
            history_size: VA推定用の行動履歴サイズ
        """
        self.vla_policy = vla_policy
        self.va_policy = va_policy
        self.lambda_cag = lambda_cag
        self.action_history = deque(maxlen=history_size)
    
    def predict(self, obs, lang_instruction):
        """
        CAG推論: VLAとVAの差分で言語conditioningを強化
        """
        # 標準VLA予測
        a_vla = self.vla_policy(obs, lang_instruction)
        
        # VA予測（モデルありorなし）
        if self.va_policy is not None:
            # 明示的VAモデル
            a_va = self.va_policy(obs)
        else:
            # 訓練不要版: 履歴から頻出行動を推定
            if len(self.action_history) > 0:
                # 最頻行動を視覚ショートカットとみなす
                a_va = self._mode_action(self.action_history)
            else:
                a_va = a_vla  # 初期ステップはVLAに従う
        
        # Counterfactual差分による調整
        a_cag = a_vla + self.lambda_cag * (a_vla - a_va)
        
        # 行動履歴更新
        self.action_history.append(a_vla.detach().cpu())
        
        return a_cag
    
    def _mode_action(self, history):
        """履歴から最頻行動を推定（離散化してmode計算）"""
        actions = torch.stack(list(history))
        # 簡易的にbinningして最頻値取得
        binned = (actions * 10).round() / 10  # 0.1刻み離散化
        mode_action = binned.mode(dim=0).values
        return mode_action.to(actions.device)

# 使用例
vla = PretrainedVLA()  # π0, π0.5など
va = VisionActionModel()  # 別途学習、または None
cag = CounterfactualActionGuidance(vla, va, lambda_cag=0.5)

obs = get_observation()
lang = "pick up the blue mug"
action = cag.predict(obs, lang)
execute_action(action)
```

## 6. 実装上の懸念

**計算コスト**  
- VAモデルありの場合、推論時に2モデル並列実行 → レイテンシ約1.5〜2倍（論文中の数値なし、構造から推定）
- 訓練不要版は履歴集計のオーバーヘッドのみ → 実用上negligible

**VAモデルの学習コスト**  
- 論文では既存VLAデータセットから言語情報を除去して学習 → 追加データ収集不要
- ただし、視覚ショートカットを正確にモデル化するため、VLAと同規模のモデル容量が必要 → メモリ消費2倍

**エッジ実装への課題**  
- 二分岐推論は組み込みデバイスでのリアルタイム実行に不利
- 訓練不要版でも行動履歴の管理が必要 → 状態管理の複雑化

**ハイパーパラメータ$\lambda$の調整**  
- タスク・環境依存で最適値が変動（論文では0.5を使用）
- counterfactual強度が過剰 → 視覚情報を無視して言語のみに過適合するリスク
- 自動調整機構は未提案 → 実運用では経験的チューニング必要

## 7. よくある質問（FAQ）

**Q1: counterfactual failuresとは具体的にどのような失敗か？**  
A: 「赤いマグカップを取れ」という指示に対し、訓練データで頻出する青いマグカップを取ってしまうような失敗。視覚的ショートカット（データセットバイアス）により言語指示が無視される現象。

**Q2: CAGは既存のVLAモデルにそのまま適用できるか？**  
A: 可能。訓練不要版はモデル構造の変更なしに推論時のみ適用できる。VAモデル版も既存VLAの重みは固定し、VAモジュールを別途学習すればよい。

**Q3: VAモデルはどのように学習するのか？**  
A: 既存のVLA訓練データから言語情報を除去し、視覚入力のみで行動予測するよう学習。データセット固有のバイアスを明示的にモデル化することで、counterfactual差分の計算に利用する。

**Q4: LIBERO-CFベンチマークの特徴は何か？**  
A: 視覚的に同一のシーンに対して複数の代替指示を割り当て、VLAが正しく言語指示を追従できるかを評価。従来のベンチマークでは検出困難だったcounterfactual failuresを定量化できる。

**Q5: 実環境での効果はシミュレーションと一致するか？**  
A: 論文では実環境でcounterfactual failures 9.4%減、タスク成功率17.2%向上を報告。シミュレーションより改善幅が大きく、実環境特有の視覚的曖昧性に対してCAGが有効である可能性を示唆。

## 8. 検討メモ

**自社プロジェクトへの適用案**  
- [ ] ピッキングタスクで物体色・形状の指示追従を改善（「赤い箱」vs「青い箱」の識別精度向上）
- [ ] 訓練不要版を既存デプロイ済みVLAに緊急パッチとして適用し、counterfactual failures削減効果を検証
- [ ] VAモジュールを自社データセットで学習し、ドメイン固有の視覚ショートカットを明示化

**追試すべき点**  
- [ ] $\lambda$の自動調整機構（環境・タスクごとの最適値探索、適応的調整）
- [ ] VAモデルの軽量化（MobileNet系バックボーンで推論レイテンシ削減）
- [ ] LIBERO-CF以外のベンチマーク（RLBench、Meta-World）での汎化性能評価

**未解決の疑問**  
- [ ] マルチモーダル入力（触覚・力覚）がある場合のCAG拡張可能性
- [ ] 連続行動空間での$(a_{\text{VLA}} - a_{\text{VA}})$の幾何的解釈と最適化手法
- [ ] 言語指示の曖昧性が高い場合（「適当な物体を取れ」）のCAG挙動

---
> **原論文**: [When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs](https://arxiv.org/abs/2602.17659v1)
