---
Title: "【論文読み】SARAH: Spatially Aware Real-time Agentic Humans"
Category:
- 論文読み
- Computer Vision
Draft: true
EditURL: https://blog.hatena.ne.jp/okojoai/okojoai.hatenablog.com/atom/entry/17179246901357962620
---

# SARAH: Spatially Aware Real-time Agentic Humans

## 1. 3行まとめ

- VR headset上でリアルタイム動作する、ユーザー位置・視線を考慮した対話型全身モーション生成手法
- Causal transformer VAE + flow matching modelにより、音声と相手位置から空間的に整合した動作を生成 → 300 FPS以上で動作
- Gaze scoring機構とclassifier-free guidanceで、学習と推論時の視線制御を分離 → ユーザーごとの視線強度調整が可能

## 2. 何が新しいか

既存の対話型モーション生成手法は音声と動作の同期に焦点を当て、ユーザーの空間的位置や移動に応答できない。VR/telepresence用途ではエージェントがユーザーの方を向き、視線を合わせる空間認識が必須だが、これまでリアルタイム動作可能な完全causalな手法は存在しなかった。

→ SARAHは、ユーザーの軌跡(trajectory)と二者間音声(dyadic audio)を入力とし、causal transformer VAEとflow matching modelを組み合わせて空間認識を持つ全身モーションを生成。

→ Embody 3Dデータセットで最高品質の動作を達成し、non-causal baselines比3倍高速な300+ FPSで動作。VR headset上での実運用を実証。

## 3. 技術の核心

### アーキテクチャ
2段階の構成:
1. **Causal transformer VAE**: ストリーミング推論用に設計。latent tokenを時系列にインターリーブして因果的に符号化・復号
2. **Flow matching model**: VAEの潜在空間でユーザー軌跡と音声を条件として動作を生成

### Gaze scoring機構
視線の自然さと方向を学習データから獲得しつつ、推論時には任意の強度で制御可能にする。

$$
\text{score}_{\text{gaze}} = f(\text{head orientation}, \text{user position})
$$

Classifier-free guidanceを適用:

$$
\hat{\epsilon}_\theta = \epsilon_\theta(\mathbf{z}_t, c) + w \cdot (\epsilon_\theta(\mathbf{z}_t, c) - \epsilon_\theta(\mathbf{z}_t, \emptyset))
$$

- $c$: ユーザー位置・音声などの条件
- $w$: 推論時に調整可能な視線強度パラメータ

→ モデル自体は自然な空間アライメントを学習、ユーザーは好みに応じてアイコンタクトの強さを調整

### Causal設計
Transformer VAEは過去のフレームのみを参照 → リアルタイムストリーミングが可能。Non-causal手法は全系列が揃うまで待機が必要。

## 4. 既存手法との比較

| 手法名 | アプローチ | 空間認識 | リアルタイム性 | 備考 |
|--------|-----------|---------|---------------|------|
| [MDM](https://arxiv.org/abs/2209.14916) | Diffusion model for human motion | なし | 非リアルタイム | 汎用動作生成、音声条件なし |
| [TalkShow](https://arxiv.org/abs/2309.10430) | Speech-driven gesture generation | なし | 非リアルタイム | 音声と動作の同期のみ、ユーザー位置非考慮 |
| [Embody](https://arxiv.org/abs/2401.08301) | Audio to photoreal embodiment | 限定的 | 非リアルタイム | 高品質だがnon-causal、VR headset非対応 |
| [MHR](https://arxiv.org/abs/2511.15586) | Parametric body model with rig | N/A | リアルタイム | モーション生成手法ではなく身体モデル、本論文で使用 |
| **SARAH (本論文)** | Causal VAE + flow matching | あり | 300+ FPS | 完全causal、VR headset実装済み |

## 5. 実装コード例

```python
import torch
import torch.nn as nn

class CausalTransformerVAE(nn.Module):
    """ストリーミング推論用のcausal transformer VAE"""
    def __init__(self, d_model=512, nhead=8, num_layers=6, latent_dim=256):
        super().__init__()
        # Causal mask付きtransformer
        self.encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model, nhead, batch_first=True),
            num_layers
        )
        self.decoder = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model, nhead, batch_first=True),
            num_layers
        )
        self.fc_mu = nn.Linear(d_model, latent_dim)
        self.fc_logvar = nn.Linear(d_model, latent_dim)
        
    def encode(self, motion_seq, causal_mask):
        """過去フレームのみを参照"""
        encoded = self.encoder(motion_seq, mask=causal_mask)
        mu = self.fc_mu(encoded)
        logvar = self.fc_logvar(encoded)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

class FlowMatchingModel(nn.Module):
    """ユーザー軌跡と音声を条件とするflow matching"""
    def __init__(self, latent_dim=256, cond_dim=128):
        super().__init__()
        self.time_embed = nn.Sequential(
            nn.Linear(1, 256),
            nn.SiLU(),
            nn.Linear(256, 256)
        )
        self.cond_encoder = nn.Linear(cond_dim, 256)
        self.net = nn.Sequential(
            nn.Linear(latent_dim + 256 + 256, 512),
            nn.SiLU(),
            nn.Linear(512, 512),
            nn.SiLU(),
            nn.Linear(512, latent_dim)
        )
    
    def forward(self, z_t, t, user_trajectory, audio_feat):
        """
        z_t: 時刻tのlatent
        t: diffusion timestep
        user_trajectory: ユーザー位置軌跡
        audio_feat: 音声特徴
        """
        # 条件を結合
        cond = torch.cat([user_trajectory, audio_feat], dim=-1)
        cond_emb = self.cond_encoder(cond)
        t_emb = self.time_embed(t.unsqueeze(-1))
        
        # Flow matchingのvelocity予測
        x = torch.cat([z_t, t_emb, cond_emb], dim=-1)
        v_pred = self.net(x)
        return v_pred

class GazeGuidance:
    """Classifier-free guidanceによる視線制御"""
    def __init__(self, model, w=1.5):
        self.model = model
        self.w = w  # 推論時に調整可能
    
    def guided_sample(self, z_t, t, user_cond, audio_feat):
        # 条件ありの予測
        v_cond = self.model(z_t, t, user_cond, audio_feat)
        # 条件なし（空間位置をマスク）
        v_uncond = self.model(z_t, t, torch.zeros_like(user_cond), audio_feat)
        # Guidance適用
        v_guided = v_uncond + self.w * (v_cond - v_uncond)
        return v_guided

# 使用例
vae = CausalTransformerVAE()
flow_model = FlowMatchingModel()
gaze_guide = GazeGuidance(flow_model, w=2.0)  # wを調整して視線強度変更
```

## 6. 実装上の懸念

### 計算コスト
- 300+ FPSを達成と記載されているが、これはどのハードウェア上での値か明示が必要。VR headset（Meta Quest等）の実機スペックでの詳細ベンチマークが不明
- Flow matching modelの推論ステップ数の記載なし → ステップ数削減時の品質劣化の度合いが不明

### データ依存性
- Embody 3Dデータセットで学習 → 異なる文化圏・対話スタイルへの汎化性能は未検証
- 二者間対話に特化 → 3人以上のグループ会話への拡張は非自明

### 視線制御の限界
- Classifier-free guidanceのweight $w$ の適切な範囲が不明 → 過度に大きいと不自然な動作の可能性
- Gaze scoring機構の具体的な実装（head orientationとuser positionからのスコア算出方法）が抽象的

### エッジ実装
- Causal transformer VAEとflow matching modelの2段階構成 → モデルサイズとメモリフットプリントの具体値が記載なし
- ストリーミング推論時のlatency（end-to-end遅延）の定量評価が不足

## 7. よくある質問（FAQ）

### Q1: Non-causal手法と比較して品質の劣化はあるか？
A: 論文ではEmbody 3Dデータセットでstate-of-the-art motion qualityを達成と主張。Non-causal baselinesと同等以上の品質を保ちつつ3倍高速と記載されているが、具体的な定量指標（FID, Frechet Gesture Distance等）の数値比較が論文要約には含まれていない。詳細は本文Table参照が必要。

### Q2: リアルタイム動作はどのハードウェアで実現されているか？
A: VR headset上での実装が実証されているとあるが、具体的なデバイス名（Meta Quest Pro等）やGPU/CPU仕様は要約に記載なし。300+ FPSは一般的にデスクトップGPU相当の性能を示唆するが、モバイルVRでの実測値かは不明。

### Q3: 既存の音声駆動動作生成手法（TalkShow等）との併用は可能か？
A: アーキテクチャ上、音声特徴とユーザー軌跡を条件とするため、音声エンコーダ部分は交換可能と推測される。ただし、空間認識を学習するには対応するデータセット（ユーザー位置とモーションのペア）が必要。既存手法のモデルをそのまま使用することは困難。

### Q4: Classifier-free guidanceのweight調整はユーザーがリアルタイムで変更できるか？
A: 推論時のパラメータであるため技術的には可能。VR UIでスライダー等を提供すればユーザーが好みの視線強度を即座に調整できる設計と考えられる。ただし、実装例やユーザースタディの詳細は要約には含まれていない。

### Q5: グループ会話（3人以上）への拡張は可能か？
A: 現手法は二者間(dyadic)音声とユーザー単一位置を想定。複数ユーザーへの拡張には、複数軌跡の条件付けと注意配分機構（誰を見るか）の追加が必要。アーキテクチャの根本的な拡張が必要で、単純な条件追加では不十分と考えられる。

## 8. 検討メモ

### 自社適用案
- [ ] VR会議システムでのアバター動作生成に適用。特に遠隔医療・教育分野での没入感向上に有効か検証
- [ ] 視線制御パラメータのA/Bテスト実施 → 日本市場でのアイコンタクト許容度測定
- [ ] 既存の音声駆動ジェスチャ生成パイプラインとの統合可能性を調査

### 追試すべき点
- [ ] モバイルVR（Quest 3等）での実測FPSとlatency測定。バッチサイズ1での動作確認
- [ ] Classifier-free guidanceのweight $w \in [0, 5]$ での動作品質の系統的評価
- [ ] 自社収集データ（日本語対話）でのfine-tuning実験 → 文化的差異の影響評価
- [ ] Flow matchingのステップ数削減（1-step distillation等）による高速化余地の検証

### 未解決の疑問
- [ ] Causal VAEのlatent token interleaving手法の詳細（論文本文要確認）
- [ ] Gaze scoring機構の損失関数設計とバランス調整方法
- [ ] 長時間対話（30分以上）での動作の多様性維持とrepetition回避策
- [ ] ユーザーが急速移動した場合のエージェント応答のスムーズさと遅延

---
> **原論文**: [SARAH: Spatially Aware Real-time Agentic Humans](https://arxiv.org/abs/2602.18432v1)
