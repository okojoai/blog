---
Title: "【論文読み】SWE-Protégé: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents"
Category:
- 論文読み
- Machine Learning
Draft: true
EditURL: https://blog.hatena.ne.jp/okojoai/okojoai.hatenablog.com/atom/entry/17179246901359005936
---

## 1. 3行まとめ

- 小型言語モデル（SLM）がSWE-benchで陥るアクションループを、強力なエキスパートモデルとの選択的協調で解決
- Qwen2.5-Coder-7B を教師あり学習＋強化学習で後学習 → SWE-bench Verified で42.4% Pass@1（従来SLM比+25.4%）
- エキスパート呼び出しは平均4回/タスク、全トークンの11%のみ → 小型モデルの実用性とコスト優位を維持

## 2. 何が新しいか

**既存の課題**  
小型言語モデルは長期ソフトウェア修正タスク（SWE-bench等）で、同じ行動を繰り返すループに陥り、解決率が低い。

**本論文のアプローチ**  
SWE-Protégéは、SLMをエキスパートモデルの「弟子」として扱い、行き詰まったときだけ助言を求める枠組みを導入。教師あり学習でエキスパート軌跡を学習後、強化学習でループ行動を明示的にペナルティ化し、無駄な協調も抑制。

**結果**  
Qwen2.5-Coder-7B が SWE-bench Verified で42.4%を達成。従来のSLM最高性能から+25.4%向上し、エキスパート依存も極小化（タスク当たり4回、全トークンの11%）。

## 3. 技術の核心

**3段階の後学習プロセス**

1. **Expert-Augmented SFT**: エキスパートモデル（GPT-4o、Claude 3.5 Sonnet等）の軌跡にエキスパート呼び出し記録を埋め込み、SLMを教師あり学習。
2. **RL with Loop Penalty**: PPOベースの強化学習で、以下の報酬関数を最適化:

$$
R = R_{\text{task}} - \lambda_{\text{loop}} \cdot C_{\text{loop}} - \lambda_{\text{collab}} \cdot C_{\text{collab}}
$$

   - $R_{\text{task}}$: タスク成功報酬（テスト通過で+1、失敗で-0.1）
   - $C_{\text{loop}}$: 5ステップ以内の繰り返しアクション数
   - $C_{\text{collab}}$: タスク失敗時のエキスパート呼び出し回数
   - $\lambda_{\text{loop}}=0.1$, $\lambda_{\text{collab}}=0.05$ で調整

3. **RL with Collaboration Penalty**: タスク失敗時に無駄なエキスパート利用を抑制する項を追加。

**エキスパート協調のメカニズム**  
SLMは特殊トークン `<<collaborate>>` でエキスパートに質問を投げ、返答を次の行動に反映。エキスパートは履歴を見て助言を返すが、最終決定権はSLMに残る。

## 4. 既存手法との比較

| 手法名 | アプローチ | SWE-bench Verified Pass@1 | 特徴・備考 |
|--------|-----------|---------------------------|-----------|
| [SWE-agent](https://arxiv.org/abs/2405.15793) | エージェントループ + LM | ~17%（7B級） | Agent-Computer Interface で構造化したが、SLMではループ問題が顕著 |
| [OpenHands](https://github.com/All-Hands-AI/OpenHands) | マルチエージェント協調 | ~20%（小型構成） | 複数エージェント間の調整コストが高く、SLMでは効果限定的 |
| [Agentless](https://arxiv.org/abs/2407.01489) | ループレス・階層的修正 | ~27%（大型LM使用時） | ループは回避できるが、SLMではヒューリスティック選択が不安定 |
| [AutoCodeRover](https://arxiv.org/abs/2404.05427) | コード構造理解重視 | 15〜20%（7B級） | リポジトリ探索に時間を要し、長期タスクで方向喪失 |
| **SWE-Protégé（本論文）** | 選択的エキスパート協調 + RL | **42.4%** | エキスパート利用は11%トークンのみ、ループペナルティで安定化 |

## 5. 実装コード例

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.distributions import Categorical

# エキスパート協調機能を持つSLMエージェント
class SWEProtegeAgent:
    def __init__(self, model_name, expert_model):
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.expert = expert_model
        self.collab_token = self.tokenizer.encode("<<collaborate>>")[0]
    
    def act(self, observation, history):
        # SLMが次のアクションを生成
        inputs = self.tokenizer(observation, return_tensors="pt")
        logits = self.model(**inputs).logits[:, -1, :]
        probs = torch.softmax(logits, dim=-1)
        action_id = Categorical(probs).sample()
        
        # エキスパート呼び出しをチェック
        if action_id == self.collab_token:
            # エキスパートに状況を送って助言を得る
            expert_advice = self.expert.query(observation, history)
            # 助言を次の入力に追加
            next_obs = observation + f"\n[Expert]: {expert_advice}"
            return self.act(next_obs, history + [expert_advice])
        
        return self.tokenizer.decode(action_id)
    
    def compute_rl_reward(self, trajectory, task_success):
        # タスク報酬
        R_task = 1.0 if task_success else -0.1
        
        # ループペナルティ：5ステップ以内の重複行動をカウント
        loop_count = 0
        for i in range(len(trajectory) - 5):
            if trajectory[i] == trajectory[i+5]:
                loop_count += 1
        
        # 協調ペナルティ：失敗時のエキスパート呼び出し回数
        collab_count = sum(1 for a in trajectory if "<<collaborate>>" in a)
        collab_penalty = collab_count if not task_success else 0
        
        return R_task - 0.1 * loop_count - 0.05 * collab_penalty

# PPOで後学習（擬似コード）
def train_with_ppo(agent, env, episodes=1000):
    optimizer = torch.optim.Adam(agent.model.parameters(), lr=1e-5)
    
    for ep in range(episodes):
        obs = env.reset()
        trajectory, log_probs = [], []
        
        for step in range(100):  # 最大100ステップ
            action = agent.act(obs, trajectory)
            obs, done = env.step(action)
            trajectory.append(action)
            # log_probsを記録してPPO更新
            if done:
                break
        
        reward = agent.compute_rl_reward(trajectory, env.task_passed())
        # PPO loss: clip(ratio, 1-ε, 1+ε) * advantage
        loss = compute_ppo_loss(log_probs, reward)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

## 6. 実装上の懸念

**学習コスト**  
- SFTは300M行動ステップ、RLは1,000エピソード → 8×H100で約120時間（推定）
- エキスパート軌跡の収集に先行コストが発生（GPT-4o APIコール）

**メモリ消費**  
- 7Bモデルでも履歴が長いタスク（平均100ステップ）ではコンテキスト長が16k〜32kトークンに達する → バッチサイズを小さくする必要

**エキスパート依存性**  
- エキスパートモデルの品質が最終性能に直結 → GPT-4o/Claude 3.5が使えない環境では効果が不明
- エキスパート呼び出しが11%トークンでもAPIコストは無視できない（1タスク平均数百トークン）

**エッジ実装の課題**  
- オンデバイスでエキスパートを呼び出す構成は非現実的 → クラウドハイブリッド構成が前提
- レイテンシはSLM単体より悪化（エキスパート呼び出し時に数秒待機）

## 7. よくある質問（FAQ）

**Q1. エキスパートモデルなしでSLM単体でも性能向上するか？**  
A1. 論文のアブレーションでは、エキスパート協調なしのRLのみでも+10%程度の改善が見られた。しかし、42.4%の達成にはエキスパート軌跡の学習が不可欠であり、単体運用は推奨されない。

**Q2. どのタイミングでエキスパートを呼び出すか？**  
A2. SLMが `<<collaborate>>` トークンを生成した時点で自動呼び出し。RLで「行き詰まり検知」を学習するため、無駄な呼び出しは訓練過程で抑制される。平均4回/タスク（100ステップ中）に収束。

**Q3. 他のSLM（Llama 3.1-8B等）にも適用可能か？**  
A3. 原理的には可能。論文はQwen2.5-Coder-7Bで実験したが、コード理解能力の高いベースモデルであれば同様の後学習プロセスを適用できる。ただし、初期性能が低いモデルでは収束に時間がかかる可能性がある。

**Q4. SWE-bench Lite や Full では性能はどうか？**  
A4. SWE-bench Verified（最難関300件）で42.4%。Lite（簡易版）では未評価だが、同手法でより高い解決率が期待できる。Full（2,294件）は計算コストの都合で未実施。

**Q5. エキスパートモデルを複数併用することは可能か？**  
A5. 論文では単一エキスパートを使用。複数エキスパートの投票や切り替えはアーキテクチャ上可能だが、ルーティングロジックの学習が必要で、トークンコストも増加する。

## 8. 検討メモ

- **自社適用案**: 社内コードベースでのバグ修正エージェントにSWE-Protégéを適用。エキスパートは社内で fine-tune した GPT-4o ベースモデルを使用し、機密情報漏洩を防ぐ。
- **追試項目**:
  - Llama 3.1-8B や DeepSeek-Coder-7B での再現実験
  - エキスパート呼び出し閾値（現在4回）を動的調整する機構の追加
  - エッジ環境向けに量子化（INT4）したSLM+クラウドエキスパートのハイブリッド構成
- **未解決の疑問**:
  - エキスパートがバイアスのある助言をした場合、SLMは正しく無視できるか？
  - タスク失敗時の協調ペナルティ（$\lambda_{\text{collab}}$）の最適値は環境依存か？
  - 100ステップ超の超長期タスクではループペナルティのみで対処可能か、別途メモリ機構が必要か？

---
> **原論文**: [SWE-Protégé: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents](https://arxiv.org/abs/2602.22124v1)
